# TeamBlind Interview Post Summaries - Batch 04

## Need help preparing for Applied ML Scientist Interview at Etsy
**URL:** https://www.teamblind.com/post/need-help-preparing-for-applied-ml-scientist-interview-at-etsy-zrt44zce
**Summary:** A candidate asks for help preparing for an Applied ML Scientist role at Etsy. The recruiter indicated the interview focuses on working with existing ML code in Python/TensorFlow inside a Jupyter Notebook environment, with no LeetCode-style questions. The emphasis is on hands-on ML coding, debugging using documentation, and communicating technical ideas effectively.
**Interview Questions Mentioned:**
- ML model evaluation approaches
- Scalability of ML models in production
- Debugging ML errors using documentation
- Discussing ML practicalities such as edge cases
**Key Insights:**
- Interview uses Jupyter Notebook environment, not a traditional coding platform
- No LeetCode-style algorithmic questions; focuses on practical ML coding
- Topics include evaluation, scalability, and edge cases in ML systems
- TensorFlow/Python proficiency is expected
- Communication of technical ideas is explicitly evaluated

---

## Need help with preparing for Google Machine Learning interviews
**URL:** https://www.teamblind.com/post/need-help-with-preparing-for-google-machine-learning-interviews-sxoqv75e
**Summary:** A post from someone preparing for Google ML interviews seeking advice and resources. Community responses from a Google L6 MLE with 7+ years of experience and a PhD outline the preparation strategy for ML system design, coding, and behavioral rounds. The advice centers on end-to-end ML system knowledge and LeetCode pattern practice.
**Interview Questions Mentioned:**
- ML system design covering data sources, feature building, model building, offline/online evaluation, and deployment
- LeetCode medium and hard problems (Neetcode 150 / LC 75 list)
- Behavioral questions simulated using ChatGPT
**Key Insights:**
- Google ML interviews require end-to-end ML system design knowledge (not just modeling)
- Coding prep: focus on Neetcode 150 and LC 75, emphasizing patterns over memorization
- Behavioral prep benefits from mock interview simulation
- ML system design is a distinct and major interview component at Google
- Recommended to practice mock interviews specifically for the ML system design round

---

## Nvidia Deep Learning Software Engineer Interview
**URL:** https://www.teamblind.com/post/Nvidia-Deep-Learning-software-engineer-interview-gmhhWGJV
**Summary:** Candidates share details of the NVIDIA Deep Learning Software Engineer interview process, which typically includes two phone screens followed by an onsite. The first phone screen involves live coding, the second has in-depth technical questions, and the onsite covers approximately 5 coding or technical questions over 6 hours. Topics range from LeetCode-style problems to deep learning specifics like implementing multi-head attention or BPE tokenization.
**Interview Questions Mentioned:**
- Implement multi-head attention
- Implement BPE (Byte Pair Encoding)
- LeetCode-style coding/data structures problems
- CUDA or OpenCL knowledge for algorithm engineer roles
- Compiler/optimization questions (e.g., TensorRT, graph parsers)
- C++/Python mixed questions
**Key Insights:**
- Interview structure: 2 phone screens + onsite (up to 6 hours, ~5 questions)
- First phone screen includes a live coding session
- Role determines mix: some positions emphasize C++ and compiler work, others are more Python/ML focused
- Inference framework knowledge (TensorRT, graph parsers) can be relevant depending on role
- Candidates rated interview difficulty approximately 3.6/5

---

## NVIDIA MLE Interview Process
**URL:** https://www.teamblind.com/post/NVIDIA-MLE-Interview-process-UjiGc64u
**Summary:** A discussion thread about the NVIDIA Machine Learning Engineer interview process. The process starts with a 30-minute recruiter screen, then proceeds through multiple rounds including coding challenges, system design, and ML case studies. One candidate reported going through 7 rounds (2 non-technical, 5 technical), while another described 8 panel rounds including a debug-focused coding problem.
**Interview Questions Mentioned:**
- Gini impurity and comparisons between Random Forest and SVM
- Streaming and real-time data ingestion architectures
- Data architecture questions
- GPU optimization and model scaling
- ML system architecture design
- Debug problem requiring all test cases to pass
**Key Insights:**
- Recruiter screen is 30 minutes covering background and light behavioral/technical questions
- Total round count varies: 7-8 rounds reported by candidates
- Technical rounds cover deep dives into GPU optimization, model scaling, and system architecture
- Coding rounds may include debugging problems rather than standard LeetCode
- Behavioral questions include: why NVIDIA, disagreements with team members, cultural alignment

---

## Nvidia ML Interview Experience
**URL:** https://www.teamblind.com/post/Nvidia-ML-interview-experience-exGUqrY8
**Summary:** Candidates share ML interview experiences at NVIDIA covering the full loop. The process typically starts with a recruiter call, then an online assessment, followed by ML-focused final rounds that include ML case studies and deep dives into past projects. Questions span classical ML theory, computer vision topics, and evaluation methodology.
**Interview Questions Mentioned:**
- How does gradient descent work? Can it reach global optimum?
- Loss plane guarantees with infinitely small learning rates
- Classical CV dataset sizes and image dimensions
- Efficient methods for downloading and reading large datasets
- How to evaluate ML experiment results
- Common MLLM benchmark metrics
**Key Insights:**
- Process: recruiter call → online assessment → final ML rounds (2 rounds)
- Final rounds focus on past ML project experience and practical ML case studies
- Theoretical ML questions cover gradient descent, optimization, and convergence properties
- CV-specific knowledge is tested for relevant roles (dataset sizes, image dimensions)
- LLM/MLLM benchmark knowledge is increasingly relevant
- Behavioral component includes culture fit and motivation questions

---

## NVIDIA Sr Deep Learning Software Engineer Interview
**URL:** https://www.teamblind.com/post/nvidia-sr-deep-learning-software-engineer-interview-2b4c5dnw
**Summary:** Candidates share experiences interviewing for the Senior Deep Learning Software Engineer role at NVIDIA. The interview includes a 20-minute experience deep dive followed by lightweight system design discussion, then approximately 30 minutes with 2 medium-difficulty LeetCode-style problems (not taken from LeetCode directly). C++ is a major component for this role, and interviewers deeply probe what candidates have worked on.
**Interview Questions Mentioned:**
- 2 medium-level LeetCode-style coding problems (novel problems, not from LeetCode)
- Implement multi-head attention
- Implement BPE tokenization
- Deep dive on prior projects and deep learning research
**Key Insights:**
- Format: ~20 min experience/system design deep dive + ~30 min coding (2 medium problems)
- C++ knowledge is significant for this role
- Interviewers grill candidates on their prior work; deep technical knowledge of past projects is essential
- Coding problems are custom, not taken directly from LeetCode
- Deep learning research background is a strong advantage
- Recruiter cannot provide specific question details in advance; preparation must be broad

---

## OpenAI/Anthropic Interview Difficulty Compared to Meta
**URL:** https://www.teamblind.com/post/openaianthropic-interview-difficulty-compared-to-meta-gpptpuwo
**Summary:** A comparison thread discussing interview difficulty at OpenAI and Anthropic versus Meta. The consensus is that Meta is more predictable and LeetCode-memorization-friendly, while OpenAI focuses on practical programming and refactoring questions with multiple twists. Anthropic's initial CodeSignal assessment is described as the most difficult initial coding round encountered, though the problems reflect real-world scenarios.
**Interview Questions Mentioned:**
- OpenAI: programming and refactoring questions with several twists on a single problem
- Anthropic: CodeSignal assessment with 4 challenges in 90 minutes combining algorithms and React UI questions
**Key Insights:**
- Meta is considered more beatable due to structured LeetCode preparation
- OpenAI has "a really high bar" with practical, non-LeetCode questions; one question but with multiple layers of complexity
- Anthropic's CodeSignal is described as the hardest initial coding round but most practically relevant
- OpenAI de-emphasizes traditional LeetCode in favor of practical problem-solving
- Meta has more available resources to prepare; OpenAI/Anthropic are less predictable

---

## OpenAI / Anthropic Interviews
**URL:** https://www.teamblind.com/post/OpenAI-Anthropic-interviews-3KFEWuPE
**Summary:** A detailed discussion of the OpenAI and Anthropic interview loops for software engineering roles. OpenAI's full loop is described as 3 rounds: code refactoring on CoderPad, a project deep-dive with a prepared slide presentation, and a behavioral/culture-fit round. Anthropic starts with a CodeSignal test, then moves to a technical assessment. Both companies place heavy emphasis on practical skills over algorithmic puzzle-solving.
**Interview Questions Mentioned:**
- OpenAI: Code refactoring round — given a single file on CoderPad, make it production-ready
- OpenAI: Implement a simplified in-memory database (levels 1-4)
- OpenAI: System design — webhook service (caching, database design, failure mechanisms, REST implementation)
- OpenAI: Project deep dive — prepare 4-5 slide presentation on prior work
- Anthropic: CodeSignal with 4 challenges in 90 minutes (algorithms + React UI)
- Anthropic: Technical assessment with LeetCode-style question involving concurrency (3 stages)
**Key Insights:**
- OpenAI's phone screen (2 rounds: coding + system design) doubles as part of the full loop — no repeat
- Full loop is relatively short: 3 rounds (code refactoring, project deep dive, behavioral)
- The project deep-dive with a slide presentation is unusual and takes ~2 hours to prepare
- Anthropic's CodeSignal is harder and more comprehensive than most companies' initial screens
- Both companies favor practical, real-world problem formats over standard LeetCode
- OpenAI coding questions come from a recurring bank of ~8 problems (CD directory, Excel sheet, In-memory DB, KV store, Resumable iterator, Node counting, GPU credit, Dependency version check)

---

## Open AI Incoming Interview
**URL:** https://www.teamblind.com/post/open-ai-incoming-interview-nj6vh3zw
**Summary:** A short post from September 2025 from someone with an upcoming OpenAI ML role interview asking for similar LeetCode-style questions to prepare. The post is primarily a preparation request rather than a debrief, with limited community responses captured in search results.
**Interview Questions Mentioned:** None specified
**Key Insights:**
- Candidate is preparing for an ML-focused role at OpenAI
- The post confirms that some ML role candidates still prepare LeetCode-style problems for OpenAI
- Limited information available from this specific thread

---

## OpenAI Interview Loop
**URL:** https://www.teamblind.com/post/openai-interview-loop-b4x2kdum
**Summary:** A candidate shares details of the OpenAI interview loop for a fullstack SWE position. The loop consists of 6 rounds: 2 coding, 2 architecture, 1 technical deep dive, and 1 behavioral. This is one of the more detailed structural breakdowns of OpenAI's full interview process available on Blind.
**Interview Questions Mentioned:** None specified beyond round types
**Key Insights:**
- Full loop: 2 coding rounds + 2 architecture rounds + 1 technical deep dive + 1 behavioral round
- Total of 6 rounds for a fullstack SWE role
- Architecture rounds are separate from coding, indicating significant weight on system design
- Technical deep dive is distinct from the project presentation described in other posts (may be role-specific variation)
- Behavioral round is standard for all loops

---

## OpenAI Intro Call / Interview Process
**URL:** https://www.teamblind.com/post/openai-intro-callinterview-process-fy7bwoyx
**Summary:** A discussion about OpenAI's initial recruiter call and what to expect in the broader interview process. The recruiter screen is approximately 30 minutes covering background, motivation, and fit. The hiring process at OpenAI is decentralized, meaning steps and styles vary significantly by role and team, but all paths include some form of technical assessment before the onsite.
**Interview Questions Mentioned:**
- "Tell me about yourself"
- "Why OpenAI?"
- "Walk me through your resume"
- "What are you looking for moving forward?"
**Key Insights:**
- Recruiter call: 30 minutes, standard background + motivation questions
- Hiring process is highly decentralized — expect variation by team and role
- Technical pre-screen can be a phone screen, asynchronous assessment, or take-home (sometimes two steps)
- Recruiter will brief you on what's coming in the next round
- Avoid revealing salary expectations or competing offers at this stage
- Being familiar with recent OpenAI product/research updates relevant to the team is important

---

## OpenAI Research Engineer Interview
**URL:** https://www.teamblind.com/post/openai-research-engineer-interview-vhbhacgs
**Summary:** A post detailing the OpenAI Research Engineer interview process. The initial screening is a 2-hour session with one ML design round and one pandas-style coding round. The subsequent onsite loop includes a deep dive, regular coding, an ML debugging round (with transformer model study recommended), and a behavioral round. Ethics and AI safety are also discussed.
**Interview Questions Mentioned:**
- ML design round (1 hour)
- Pandas-style coding round (1 hour)
- ML debugging round — study the transformer model architecture
- Behavioral questions
- Ethics and safety in AI discussion
**Key Insights:**
- Initial screening: 2 hours (1 ML design + 1 coding/pandas)
- Full loop: deep dive + coding + ML debugging + behavioral
- Transformer model knowledge is specifically recommended for the ML debugging round
- Ethics and AI safety are discussed as part of the interview — not just a formality
- OpenAI looks for well-designed solutions, high-quality code, optimal performance, and good test coverage
- All questions have a degree of practicality and are language-agnostic

---

## Open AI Research Engineer - ML System Design Interviews
**URL:** https://www.teamblind.com/post/open-ai-research-engineer-ml-system-design-interviews-ipiq7yo4
**Summary:** A candidate preparing for the OpenAI Research Engineer ML system design interview round asks for example questions and evaluation rubrics. One commenter confirmed going through this round, receiving mid-level feedback but still advancing to onsite. The design question covered search and recommendation systems with LLMs, focusing on retrieval, ranking, and adapting LLMs to interact with such systems.
**Interview Questions Mentioned:**
- ML system design: "Search/Recommendation Systems with LLMs" — experience with search, ranking, retrieval, and adapting LLMs to interact with such systems
- Alternative topic mentioned: semi-supervised learning system design
**Key Insights:**
- ML system design round is part of the Research Engineer loop at OpenAI
- The round tests applied ML system architecture, not just theoretical knowledge
- Mid-level performance on this round can still result in advancing to onsite
- LLM integration into classical ML systems (search, recommendation) is a current focus area
- Candidates can sometimes choose between design topics if given a choice

---

## OpenAI Software Engineering Interview Questions
**URL:** https://www.teamblind.com/post/OpenAi-software-engineering-interview-questions-GoXb5YWx
**Summary:** A thread asking about technical and programming questions for non-AI software engineering roles at OpenAI. Community responses reveal that OpenAI uses a recurring set of approximately 8 known coding problems delivered through CoderPad, focused on building or refactoring practical systems rather than LeetCode-style algorithmic puzzles.
**Interview Questions Mentioned:**
- CD directory implementation
- Excel sheet simulation
- In-memory database (build/extend across multiple levels)
- Key-value store implementation
- Resumable iterator
- Node counting
- GPU credit system
- Dependency version check
- Data structure for creating, updating, deleting from a map with sorting and merging requirements
- System design: webhook service (caching, database design, failure mechanisms, REST)
**Key Insights:**
- OpenAI has a known, recurring bank of ~8 coding problem types for SWE roles
- Problems are practical system-building exercises, not typical LeetCode puzzles
- First technical phone screen is ~1 hour on CoderPad, algorithms and data structures style but applied
- No IDE setup required; CoderPad is used for all coding rounds
- Questions are more representative of real engineering work than competitive programming

---

## OpenAI TPM Interview Experience
**URL:** https://www.teamblind.com/post/openai-tpm-interview-experience-jjtzwccs
**Summary:** A detailed debrief of the OpenAI Technical Program Manager (TPM) interview process for a role on the Safety Systems team. The loop includes a STAR-format behavioral round, a scenario-based TPM process design round (launching a model under safety constraints), and a hiring manager/values round focused on working style and motivation.
**Interview Questions Mentioned:**
- STAR: "Describe a time you aligned conflicting priorities across teams"
- STAR: "How do you translate between technical and non-technical stakeholders?"
- STAR: "How do you manage through ambiguity?"
- Scenario: "How would you manage launching a new model under safety constraints and tight timelines? Structure the process."
- Values: "How do you navigate ambiguity?"
- Values: "What kind of team culture do you thrive in?"
- Values: "What drives you to want to join OpenAI?"
**Key Insights:**
- TPM interviews are more structured/process-design-oriented than typical PM interviews
- Safety Systems team context means AI safety considerations are woven into scenario questions
- Structured thinking, stakeholder mapping, and risk identification are explicitly evaluated
- Values alignment and motivation for joining OpenAI are given significant weight
- The scenario round is a process design exercise, not a product sense question
- STAR format is expected for behavioral rounds

---

## Palantir Forward Deployed AI Engineer Interview
**URL:** https://www.teamblind.com/post/palantir-forward-deployed-ai-engineer-interview-w4mqdxvc
**Summary:** A candidate who received a recruiter outreach for the Palantir Forward Deployed AI Engineer role asks whether it resembles the standard FDSE (Forward Deployed Software Engineer) interview. Community responses suggest the process is similar to FDSE, consisting of a phone screen followed by 3 interview rounds covering a use case walkthrough, a learning/decomposition facet, and a hiring manager round.
**Interview Questions Mentioned:** None specific
**Key Insights:**
- Forward Deployed AI Engineer interview appears similar to the standard FDSE interview process
- Post-phone-screen rounds: (1) work through a use case, (2) learning facet — how you pick up new technologies and decompose problems, (3) hiring manager round
- Role is distinct from standard SWE; it involves client-facing deployment of AI solutions
- Candidate was cold-contacted by a recruiter, suggesting Palantir actively recruits for this role
- Limited public information available specifically for the "AI Engineer" variant versus standard FDSE

---

## Perplexity Interview
**URL:** https://www.teamblind.com/post/Perplexity-Interview-ZnS2oiBC
**Summary:** A candidate at Google (L3, 2 years experience) received a recruiter outreach from Perplexity and asks whether it is a good lateral move and how it compares to OpenAI and Anthropic. The community response is skeptical: Perplexity is seen as overvalued, lacking a strong technical moat, and paying less than comparable OpenAI/Anthropic positions. The thread is more about company evaluation than interview specifics.
**Interview Questions Mentioned:** None
**Key Insights:**
- Perplexity's main technical advantage (web crawling and embedding) is not seen as a strong moat by the Blind community
- Perplexity does not develop in-house LLMs at the same scale as OpenAI/Anthropic
- Compensation at Perplexity is perceived as lower than equivalent roles at OpenAI or Anthropic
- Perplexity interview process includes 3 stages testing problem-solving and communication
- DS/research roles have 5-6 rounds including recruiter screen, technical/case interviews
- Primary language is Python
- Thread serves as a company evaluation discussion more than interview prep

---

## Rejected at Amazon Applied Scientist Interview
**URL:** https://www.teamblind.com/post/Rejected-at-Amazon-Applied-Scientist-Interview-0ibCPh1u
**Summary:** A fresh M.Tech graduate from a top Indian university shares their Amazon Applied Scientist interview experience after receiving a referral. They passed the ML phone screen but were rejected after the onsite, which included ML breadth, ML depth/system design, and a coding round. The coding round contained 4 problems (DP, Array, String) in 1 hour, and the candidate solved 3. A 6-month cooldown period applies after rejection.
**Interview Questions Mentioned:**
- Phone screen: basic ML questions
- ML Breadth round: general ML questions including mathematical derivations
- ML Depth round: thesis discussion and scalable system design
- Coding round: 4 questions (DP, Array, String) at Easy-Medium LeetCode level in 1 hour
**Key Insights:**
- Amazon AS loop: phone screen → ML breadth → ML depth/system design → coding
- All rounds conducted remotely (Amazon Chime with screen sharing for derivations)
- Mathematical derivations are part of the ML breadth round
- Coding round has 4 questions in 1 hour — faster than typical LeetCode sessions
- Community comment: Amazon ML interviews are noted as "very random" — interviewers tend to ask about what they personally know
- 6-month cooldown after rejection before reapplying

---

## Roku India MLE Interview Experience
**URL:** https://www.teamblind.com/post/roku-india-mle-interview-experience-phfzkxte
**Summary:** A candidate shares a detailed and positive MLE interview experience at Roku Bangalore. The process was initiated through a LinkedIn application with a quick recruiter response (3-5 days). Four technical rounds covered ML fundamentals, ML design, coding/SQL, and additional ML design, followed by two hiring manager rounds. The primary focus throughout was ML design.
**Interview Questions Mentioned:**
- R2 (ML Design): deep dive on one of the candidate's own past projects relevant to the role
- R3 (Coding + ML Design): simple SQL assignment + design a news article recommendation system for Yahoo News landing page
- R4: coding-focused round
**Key Insights:**
- Application via LinkedIn with fast recruiter turnaround (3-5 days)
- ML design is the dominant focus across multiple rounds
- Practical project relevance is valued — candidates discuss their own past work in depth
- SQL is included even for MLE roles at Roku
- News/media recommendation system design is a representative question for Roku's domain
- Two separate hiring manager rounds after technical rounds
- Community confirms: ML breadth and ML design depth are the key preparation areas

---

## Scale AI Interview Process
**URL:** https://www.teamblind.com/post/scale-ai-interview-process-rcptnu3a
**Summary:** A general inquiry post asking for recent experiences with the Scale AI software engineer interview process. Community responses indicate the process includes a take-home assignment, technical screen, behavioral round, and coding test. Some candidates report no LeetCode. One AI engineering role involved an OOP coding problem based on a card game implementation.
**Interview Questions Mentioned:**
- OOP coding problem: implement a card game with N players under specified rules
- System design: potentially high-level (e.g., designing Twitter) or low-level (e.g., load balancer)
**Key Insights:**
- Scale AI interview stages: take-home assignment → tech screen → behavioral → coding test
- Some roles reportedly do not use LeetCode
- The card game OOP problem appears to be a recurring Scale AI coding challenge
- System design round scope (high vs. low level) varies by role
- The tech screen evaluates both technical depth and project experience
- Process is relatively standard SWE loop for an AI company

---

## Scale AI Machine Learning Research Engineer / Scientist Interview
**URL:** https://www.teamblind.com/post/scale-ai-machine-learning-research-engineer-scientist-interview-tjnnfk1e
**Summary:** A detailed account of the Scale AI Machine Learning Research Engineer/Scientist interview, covering ML fundamentals, computer vision knowledge, Python coding, LLM debugging, and behavioral questions. The process tests both breadth (classical ML, CV, transformers) and applied coding ability (graph algorithms, LLM optimization).
**Interview Questions Mentioned:**
- ML fundamentals: dealing with overfitting
- Describe CNN and ViT architectures — advantages, disadvantages, and when to use for classification, detection, and segmentation
- Methods to map 3D points to 2D space
- Research paper summary: masked transformers — summarize content, innovations, model input/output
- Python coding: build a graph from a Python dictionary, then find shortest distance between two nodes (framed in Scale AI context)
- LLM coding: debug and optimize LLM-related code
- Behavioral: most meaningful project, what motivates/demotivates you, biggest weakness, stepping outside comfort zone
**Key Insights:**
- Interview spans classical ML, computer vision, transformers, and LLM engineering
- Reading and summarizing a research paper is a live interview task — strong paper-reading skills required
- Graph algorithms (BFS/Dijkstra) are tested in a practical, domain-relevant framing
- LLM debugging and optimization is a specific technical component
- Behavioral questions are thorough and probe self-awareness and motivation
- Role requires both research breadth and engineering depth

---

## Scale AI MLE
**URL:** https://www.teamblind.com/post/scale-ai-mle-5cgmj3pd
**Summary:** A candidate shares their Scale AI MLE onsite experience from a few years prior. The process included a take-home assignment (training a small model), followed by onsite rounds covering broad ML questions, TensorFlow code completion in Google Colab, and additional ML assessment. The candidate found the process unclear in its structure and noted the TensorFlow requirement was a surprise for a PyTorch practitioner.
**Interview Questions Mentioned:**
- ML questions across different topics: distributions and when to use which
- Complete existing TensorFlow code in Google Colab
- Take-home: train a tiny model for a given problem (not discussed in onsite)
**Key Insights:**
- Take-home assignment exists but is used only as a filter to reach onsite — not discussed afterwards
- Onsite ML round covers broad ML concepts including statistical distributions
- Framework-specific coding: TensorFlow knowledge tested even for candidates who primarily use PyTorch
- Google Colab is used as the interview environment for coding rounds
- Candidates should prepare for a 4-round onsite (1 live coding + 3 coding/ML rounds)
- Structure and agenda of rounds were not communicated to the candidate in advance

---

## Scale AI Reject - My Experience
**URL:** https://www.teamblind.com/post/scale-ai-reject-my-experience-rqs2dkg4
**Summary:** A candidate shares their Scale AI rejection experience after reaching the onsite stage. The technical interview used an existing codebase with 3 related questions based on a card game; the candidate completed 2 of 3 within the time limit. The EM interview was a casual culture/vibe check about Scale AI and prior work. Rejection was attributed by the recruiter solely to coding completeness — not finishing the third question.
**Interview Questions Mentioned:**
- OOP coding: 3 related questions based on an existing card game codebase (~20 minutes each)
- EM round: casual conversation about Scale AI and the candidate's prior work experience
**Key Insights:**
- Scale AI's coding interview uses an existing codebase with 3 progressive questions, not blank-slate coding
- Speed and code completion are prioritized — the interviewer did not emphasize time/space complexity
- Getting code to produce output quickly matters more than elegant optimization
- The EM round is a "vibe check" — casual and conversational
- Rejection can occur even when the technical and EM rounds feel positive if coding completeness is insufficient
- Recruiter explicitly cited "coding completeness and quality" as the sole rejection reason
- Code style may also be evaluated alongside completeness
