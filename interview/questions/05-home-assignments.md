# Home Assignments Analysis: AI Engineering Job Market

Analysis of take-home assignments, paid work trials, and asynchronous assessments extracted from 1,765 AI/ML engineering job descriptions.

See also: [Interview Process Report](01-interview-process.md) for the full interview process analysis including company-by-company breakdowns, common patterns, and notable practices.


### Paid Work Trials

5 companies offer paid assessment alternatives: PostHog (1-day SuperDay), Lorikeet (~2-day work trial), Infinity Constellation (1-week paid trial building something real), Fifth Dimension (1-week paid trial as alternative to standard interview), and CompuGroup Medical US (3-month stipended internship with performance-based conversion).

## Overview

Of the 51 companies with disclosed interview processes, 17 (33%) include a take-home or asynchronous assignment. An additional 5 companies use paid work trials instead of traditional assignments. Only 1 company (TensorOps) explicitly states they do not use take-homes.

## Detailed Assignment Descriptions

### 1. Column Tax - Software Engineer (Applied AI)
File: [`6351158_Column_Tax_Software_Engineer_Applied_AI.yaml`](../job-market/data_raw/6351158_Column_Tax_Software_Engineer_Applied_AI.yaml)

> We'll ask you to complete a ~3-hour project that mirrors the kind of integration or tooling work we do every day. You'll have full freedom to do it your way - this is about decision-making, clarity, and implementation, not code golf.

- Duration: ~3 hours
- Format: Project mirroring real integration/tooling work
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 4 (after intro call with Head of Engineering)
- Evaluation: Discussed during 4.5-hour interview panels over 2 days


### 2. FlowFuse - Full Stack Developer (AI-focused)
File: [`8067206_FlowFuse_FlowFuse_Full_Stack_Developer_AI-focused.yaml`](../job-market/data_raw/8067206_FlowFuse_FlowFuse_Full_Stack_Developer_AI-focused.yaml)

> Candidates choose one of the following options. Both are explicitly time-boxed to 2-3 hours:
> - Option A: Build a small AI-powered feature or tool (for example: intelligent search, summarization, validation, or an assistant-style workflow) using an LLM API.
> - Option B: Contribute a small, scoped AI-related pull request or prototype demonstrating applied AI integration in an existing codebase.
> - Note: AI tools are explicitly allowed and encouraged where appropriate.

Technical interview follow-up:
> The discussion focuses on problem understanding, AI design decisions and tradeoffs, system structure, reliability considerations, and how the solution would evolve over time rather than feature completeness. There will be explicit discussion of where AI was used, how it was used, and why those choices were made.

- Duration: 2-3 hours (time-boxed)
- Format: Choice between building an AI feature or contributing a PR
- Paid: Explicitly unpaid
- AI tools: Explicitly allowed and encouraged
- When: Step 4 of 7 (after 2 calls)
- Evaluation: 60-min technical interview reviewing the work with 2-3 team members


### 3. Harmonic - Full Stack Product Engineer (AI Search)
File: [`6727775_Harmonic_Full_Stack_Product_Engineer_AI_Search.yaml`](../job-market/data_raw/6727775_Harmonic_Full_Stack_Product_Engineer_AI_Search.yaml)

Listed as "Take Home Exam" in the process - no further description of format, content, or duration provided.

- Duration: Not specified
- Format: Take-home exam (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 5 (after recruiter screening)
- Evaluation: Followed by debugging session (45 min) + jam session (1 hour)


### 4. Tenex - Applied AI Engineer / Forward Deployed AI Engineer
File: [`8095486_Tenex_tenexco_Applied_AI_Engineer.yaml`](../job-market/data_raw/8095486_Tenex_tenexco_Applied_AI_Engineer.yaml), [`7257124_Tenex_tenexco_Forward_Deployed_AI_Engineer.yaml`](../job-market/data_raw/7257124_Tenex_tenexco_Forward_Deployed_AI_Engineer.yaml)

> Take-Home: A practical challenge to showcase your technical skills.
> Team Review: Our engineers review your submission. If it's a match, we'll invite you to the final round.

> Want to skip the screen and go straight to an onsite? Prove you can build by submitting a demo via our Build First form to fast-track your application.

- Duration: Not specified
- Format: Practical challenge
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 3 of 5 (after screen call)
- Evaluation: Engineers review submission; then deep dive during onsite panel interview
- Notable: "Build First" fast-track option lets candidates skip the screen by submitting a demo upfront


### 5. Lendable - AI Engineer / Senior AI Engineer
File: [`8029593_Lendable_AI_Engineer.yaml`](../job-market/data_raw/8029593_Lendable_AI_Engineer.yaml), [`8316438_Lendable_Senior_AI_Engineer.yaml`](../job-market/data_raw/8316438_Lendable_Senior_AI_Engineer.yaml)

Listed as "Take-home exercise" followed by "Walkthrough of your exercise" - no further description of the assignment itself.

- Duration: Not specified
- Format: Take-home exercise (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 5 (after introductory call)
- Evaluation: Dedicated walkthrough interview to discuss the exercise, then meet the day-to-day team and exec team


### 6. Stacks - Staff AI Engineer
File: [`4761860_Stacks_Staff_AI_Engineer.yaml`](../job-market/data_raw/4761860_Stacks_Staff_AI_Engineer.yaml)

> Step 4: Take-home and 1-hour on-site panel interview with our team at Stacks.

- Duration: Not specified
- Format: Take-home (details not given), combined with on-site panel
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 4 of 6 (after 3 calls with talent, founder, and team member)
- Evaluation: Combined with 1-hour on-site panel interview


### 7. Melotech - AI/ML Engineer + Intern
File: [`8247431_Melotech_AIML_Engineer.yaml`](../job-market/data_raw/8247431_Melotech_AIML_Engineer.yaml), [`8247435_Melotech_AIML_Engineer_Intern.yaml`](../job-market/data_raw/8247435_Melotech_AIML_Engineer_Intern.yaml)

> Take-home case study: Real-world project - showcase your skills and working style
> Case interview: 90-minute case discussion - getting to know you & present and debate your results with a team member

- Duration: Not specified
- Format: Real-world case study project
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 1 of 6 - starts the process (no screen or intro call first)
- Evaluation: 90-minute case interview where candidate presents and debates results
- Notable: Same rigorous process for both senior and intern roles. Company is in stealth mode - candidates learn more as they progress.


### 8. Zeely - AI Prompt Engineer (ComfyUI)
File: [`7568516_Zeely_AI_Admaker_AI_Prompt_Engineer_ComfyUI.yaml`](../job-market/data_raw/7568516_Zeely_AI_Admaker_AI_Prompt_Engineer_ComfyUI.yaml)

Listed as "Test Task" in process: `Intro Call > Test Task > Final Interview > Reference check > Offer` - no further description.

- Duration: Not specified
- Format: Test task (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 5 (after intro call)


### 9. hyperexponential - Senior Technical Writer/Documentation Engineer, AI
File: [`7298439_hyperexponential_Senior_Technical_WriterDocumentation_Engineer_-_A.yaml`](../job-market/data_raw/7298439_hyperexponential_Senior_Technical_WriterDocumentation_Engineer_-_A.yaml)

> Skills Assessment - Home-based task followed by interview (60 minutes)

- Duration: Not specified (interview discussing it is 60 min)
- Format: Home-based skills assessment task
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 3 of 4 (after talent call and manager interview)
- Evaluation: Combined with 60-min follow-up interview


### 10. boam - Applied AI Engineer
File: [`8249839_boam_Applied_AI_Engineer.yaml`](../job-market/data_raw/8249839_boam_Applied_AI_Engineer.yaml)

> Work Sample: Solve a real Boam-style ML/AI challenge that shows your modeling approach, pipeline thinking, and execution muscle

- Duration: Not specified
- Format: Real company-specific ML/AI challenge
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 3 of 4 (after intro call and deep dive)
- Evaluation: Not explicitly described; followed by founder/leadership conversation


### 11. SmartAssets - AI/ML Engineer
File: [`7694512_SmartAssets_AI_Engineer_Machine_Learning_Engineer.yaml`](../job-market/data_raw/7694512_SmartAssets_AI_Engineer_Machine_Learning_Engineer.yaml)

> Successful applicants will receive a coding challenge to evaluate their programming knowledge and skills as outlined in this job description. This step is an essential part of our selection process to ensure a good match with our team's needs and the demands of the role.

- Duration: Not specified
- Format: Coding challenge
- Paid: Not mentioned
- AI tools: Not mentioned
- When: After application review (only assessment step described)


### 12. Roboflow - Full Stack Engineer, AI Agents
File: [`8027169_Roboflow_Full_Stack_Engineer_AI_Agents.yaml`](../job-market/data_raw/8027169_Roboflow_Full_Stack_Engineer_AI_Agents.yaml)

> [45m] Build a project with Roboflow and present it to our CTO, Brad Dwyer

This may involve advance preparation outside the live session (building a project with the company's product), though the presentation itself is 45 minutes live.

- Duration: ~45 min presentation (prep time not specified)
- Format: Build a project using Roboflow's product, present to CTO
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 4 of 7 (after hiring manager intro and technical assessment)
- Evaluation: Direct presentation to CTO


### 13. Wing Assistant - MERN Stack Developer (AI Agents)
File: [`8359858_Wing_Assistant_MERN_Stack_Developer_AI_Agents.yaml`](../job-market/data_raw/8359858_Wing_Assistant_MERN_Stack_Developer_AI_Agents.yaml)

Listed as "Technical Task" between the introductory call and the final interview - no further description of format, content, or duration provided.

- Duration: Not specified
- Format: Technical task (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 3 (after introductory call)
- Evaluation: Followed by 1-hour final interview with CEO, CPO & CTO


### 14. Lendable - Senior AI Software Engineer
File: [`8440447_Lendable_Senior_AI_Software_Engineer.yaml`](../job-market/data_raw/8440447_Lendable_Senior_AI_Software_Engineer.yaml)

Listed as "Take-home task" followed by "Technical interview based on the task" - confirming Lendable's consistent use of take-homes across all AI roles.

- Duration: Not specified
- Format: Take-home task (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 2 of 4 (after screening call with HM)
- Evaluation: Technical interview based on the task


### 15. Monzo Bank - Senior Staff Software Engineer, AI Customer Operations
File: [`8492197_Monzo_Bank_Senior_Staff_Software_Engineer_AI_Customer_Operat.yaml`](../job-market/data_raw/8492197_Monzo_Bank_Senior_Staff_Software_Engineer_AI_Customer_Operat.yaml)

Candidates get a choice between a take-home task or a pair coding exercise.

- Duration: Not specified
- Format: Take-home task OR pair coding (candidate's choice)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 4 of 5 (after recruiter call, initial call, system design interview)
- Evaluation: Followed by final interview including behavioral and leadership rounds
- Notable: Monzo publicly shares a senior engineer's blog post about the interview experience. ~3-4 week timeline.


### 16. Qonto - Senior Machine Learning Engineer for AI Product
File: [`8097536_Qonto_Senior_Machine_Learning_Engineer_for_AI_Product.yaml`](../job-market/data_raw/8097536_Qonto_Senior_Machine_Learning_Engineer_for_AI_Product.yaml)

> A remote or live exercise to demonstrate your skills and give you a taste of what working at Qonto could be like.

- Duration: Not specified
- Format: Remote or live exercise (candidate may choose)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: After interviews with Talent Acquisition Manager and future managers
- Evaluation: Not specified
- Notable: Average process is ~20 working days; offers follow within 48 hours of completion.


### 17. Hastings Direct - Lead Quality Engineer, Automation & AI
File: [`8463501_Hastings_Direct_Lead_Quality_Engineer_-_Automation_AI.yaml`](../job-market/data_raw/8463501_Hastings_Direct_Lead_Quality_Engineer_-_Automation_AI.yaml)

Listed as "Case study round" as the final step - no further description of format or duration.

- Duration: Not specified
- Format: Case study (details not given)
- Paid: Not mentioned
- AI tools: Not mentioned
- When: Step 3 of 3 (after recruiter screen and hiring manager intro)
- Evaluation: Not described


## Paid Work Trials (Alternative to Traditional Assignments)

5 companies offer paid assessment alternatives instead of (or alongside) traditional take-homes. PostHog runs a paid 1-day "SuperDay" of real work. Lorikeet does a ~2-day paid work trial shipping with the team. Infinity Constellation replaces all traditional interviews with a paid week building something real. Fifth Dimension offers a paid 1-week trial as an alternative to their standard interview pipeline. CompuGroup Medical US uses a 3-month stipended internship with performance-based conversion.

## Key Observations

### Format Trends
- Realistic/applied work is the dominant philosophy. Column Tax says "not code golf," boam uses "a real Boam-style ML/AI challenge," Melotech uses "real-world projects," FlowFuse offers building an AI feature or contributing actual code.
- No company uses generic algorithmic puzzles as take-homes. All assignments are applied/domain-specific.
- Choice-based assignments are growing: FlowFuse lets candidates choose between building something new or contributing a PR; Monzo Bank offers a choice between take-home and pair coding; Qonto offers remote or live exercise.
- Most companies provide minimal detail about what the assignment actually involves - only Column Tax, FlowFuse, boam, Melotech, and Qonto describe the content in any detail.

### Time Investment
- Only 2 companies specify duration: Column Tax (~3 hours) and FlowFuse (2-3 hours, time-boxed).
- The remaining 15 companies do not disclose expected time, creating uncertainty for candidates.
- Paid work trials range from 1 day (PostHog) to 1 week (Fifth Dimension, Infinity Constellation) to 3 months (CompuGroup Medical US internship).

### Compensation
- 5 companies (PostHog, Lorikeet, Infinity Constellation, Fifth Dimension, CompuGroup Medical US) explicitly pay candidates for assessment time.
- FlowFuse explicitly states theirs is unpaid.
- The remaining 13 companies don't mention compensation (likely unpaid).

### AI Tool Policies for Assignments
- 1 company explicitly allows AI (FlowFuse: "AI tools are explicitly allowed and encouraged")
- 0 companies explicitly ban AI in take-home assignments specifically (bans from Wolters Kluwer and HRT apply to live interviews)
- 16 companies don't mention AI tool policy for their assignments - a significant gap given the prevalence of AI coding assistants

### Placement in the Process
- Most assignments come after an initial screen (step 2-3 of the process)
- Melotech is unique: the case study is step 1 - candidates start with the assignment before any calls
- Monzo Bank places the take-home/pair coding at step 4, after a system design interview
- hyperexponential combines the take-home with the interview discussion in a single stage
- Stacks combines the take-home with the onsite panel

### How Assignments Are Evaluated
- Dedicated walkthrough (5 companies): Lendable, FlowFuse, Melotech, hyperexponential, and Monzo Bank all have a follow-up interview to discuss the assignment
- Team review (2 companies): Tenex has engineers review the submission before deciding on final round; Stacks combines with panel
- Presentation to leadership (2 companies): Roboflow candidates present to the CTO; Melotech candidates present and debate results
- C-suite review (1 company): Wing Assistant follows the technical task with a final interview with CEO, CPO & CTO
- Not specified (7 companies): Harmonic, Zeely, SmartAssets, boam, Qonto, Hastings Direct, and Lendable (new role) don't describe how the assignment is evaluated

### Companies That Explicitly Don't Use Take-Homes
- TensorOps - "This interview does not involve live coding or a take-home assignment"
- Glass Health - All rounds are live (no take-home)
- Clarium - Live realistic exercises with info sent in advance (no async work)
- Phare Health - Explicitly "not Leetcode," all rounds live (pseudo-coding + systems design)

## Assignments from Practitioner Interview Experiences

### OpenAI
48-hour take-home project as part of the screening phase, before the virtual onsite ([source](https://www.linkjob.ai/interview-questions/openai-loop-interview))

### Eightfold.ai
3-day window to build an AI agent demonstrating agentic behavior, natural interaction, and reasoning. This was Round 3 of 5, after an AI-conducted coding round ([source](https://medium.com/@bhardwajtushar2004/inside-eightfold-ais-agentic-ai-internship-hiring-process-2026-f86dcb625aa8))

### PromptLayer's recommended take-homes for AI Engineers
Build an agent with CSV data for email campaigns, document Q&A with citation tracking, or a code review agent. Grading prioritizes working functionality and thoughtful architecture over perfect accuracy ([source](https://blog.promptlayer.com/the-agentic-system-design-interview-how-to-evaluate-ai-engineers/))

### Gen AI Engineer take-home: blood test report processor
Round 1 was a take-home assignment: build an AI project that takes in a blood test report as a PDF, understands the medical issues, and generates suggestions by fetching content from online blog articles with source links. The catch: it had to be submitted within a few hours. The company was testing speed at which someone can understand and leverage an unfamiliar framework (the candidate had never used CrewAI before but still completed it, which impressed the interviewer in the later managerial round) ([source](https://medium.com/@khushalkumar/my-generative-ai-engineer-interview-experience-got-hired-f8a027e070b0))

### IBM AI Engineer: live coding with AI tools allowed
The speed coding test in the managerial round involved a live scenario: parse a complicated JSON file to extract a specific part following a pattern, then feed that extracted data to an AI model to get a summary. Time limit: 30 minutes. Notably, the interviewer explicitly allowed using a browser and even ChatGPT during the test. The candidate got stuck at one point but kept going and finished in time ([source](https://medium.com/@khushalkumar/my-generative-ai-engineer-interview-experience-got-hired-f8a027e070b0))

### What makes take-homes stand out
From observing 50+ AI engineer interviews: "most engineers put way too little effort into take-homes." The best ones document decisions, test edge cases, and submit with a Loom video walking through the design. One engineer built a CLI tool for summarizing PDFs with a config file so the hiring manager could test different models and chunking strategies - had two competing offers within 72 hours ([source](https://medium.com/fonzi-ai/what-ive-learned-from-sitting-in-on-50-ai-engineer-interviews-c493696453c4))

## Assignment Examples from Candidate Reports

Beyond our job description analysis, candidates report these specific assignment types on Reddit and blogs:

| Assignment type | Time | Key requirements |
|-----------------|------|------------------|
| RAG chatbot | 2-7 days | Vector DB, embeddings, citations, eval metrics |
| Agentic workflow | 2-7 days | LangGraph/LangChain, tool calling, memory, error handling |
| Full-stack AI app | 48h-7 days | React/FastAPI, LLM integration, Docker deployment |
| Calendar/task agent | 2-3 days | Function calling, Google Calendar API, state management |
| Data summarizer | 2-5 days | PDF parsing, LLM analysis, structured output |

Specific examples:
- AI-first CRM module: React/Redux frontend, FastAPI backend, LangGraph with 5+ tools, deliverable includes GitHub repo + 10-15 minute demo video. ~60 hours expected
- Conversational calendar booking agent: LangGraph for orchestration, Streamlit chat interface, FastAPI backend, Google Calendar integration
- Anthropic performance take-home: code optimization for speed, 4-hour limit originally (now open-sourced for practice), Python workload simulating TPU-like operations

## Red Flags: Unreasonable Assignments

Warning signs reported by candidates:

- Scope creep: a 72-hour "Round 1" assignment demanding full RAG + agents + UI
- Free labor: assignments that are literally the company's roadmap shipped as an interview task
- Unreasonable time limits: 45 minutes for 3 complex tasks

Reddit story (r/developpeurs, Jan 2026): one candidate was asked to build an LLM agent to ingest years of financial reports, with stock price analysis and chart generation - using only freemium APIs. Called it "an unpaid mini-consulting project" and withdrew.

### Candidate frustrations

- Multi-day take-homes followed by rejection without feedback
- Weeks of process for junior roles with low compensation
- Assignments that take 20+ hours with zero response

## Comprehensive Assignment Catalog

A catalog of concrete take-home assignments and home challenges reported by candidates, interviewers, and companies in AI/ML engineering interviews. Each entry describes what candidates were actually asked to build.


### RAG & Document Q&A

1. Blood test report processor with web-sourced suggestions
- What to build: An AI project that takes a blood test report as a PDF, understands the medical issues, and generates suggestions by fetching content from online blog articles with source links
- Time limit: A few hours (submitted same day)
- Company: Gen AI startup (unnamed)
- Tools/constraints: Candidate used CrewAI (framework they had never used before); the company was testing speed at picking up unfamiliar frameworks
- Evaluators look for: Speed of execution, ability to learn new frameworks on the fly, working end-to-end solution
- Source: [Medium - Khushal Kumar](https://medium.com/@khushalkumar/my-generative-ai-engineer-interview-experience-got-hired-f8a027e070b0)

2. Production-ready RAG customer support chatbot (system design + implementation)
- What to build: Design a production-ready customer support chatbot using RAG with open-source models. Must handle 100+ concurrent users, ground responses in company documentation (no hallucinations), keep latency under 2 seconds, track analytics on query types and user satisfaction, and remain cost-effective
- Time limit: Not specified
- Company: LLM engineer role (unnamed)
- Tools/constraints: Must use open-source models for data privacy (candidate used Llama-2 7B/13B, BGE-M3 for embeddings, Milvus vector DB, Kafka, FastAPI, React frontend)
- Evaluators look for: Architecture diagram, model selection justification, RAG implementation details, deployment strategy with infrastructure, monitoring/evaluation plan, cost estimation. Candidate scored 9/10
- Source: [Dev.to - Mai Chi Bao](https://dev.to/mrzaizai2k/how-i-aced-my-llm-interview-building-a-rag-chatbot-2p6f)

3. Document Q&A system with citation tracking for multi-hop questions
- What to build: Create a document Q&A system that tracks citations across multi-hop questions (questions that require combining information from multiple documents)
- Time limit: Not specified (take-home)
- Company: Recommended by PromptLayer as standard AI engineer assessment
- Tools/constraints: Open choice of tools
- Evaluators look for: Working functionality and thoughtful architecture over perfect accuracy; meaningful evaluation implementation is essential
- Source: [PromptLayer blog](https://blog.promptlayer.com/the-agentic-system-design-interview-how-to-evaluate-ai-engineers/)

4. CLI tool for summarizing long PDFs with configurable models and chunking
- What to build: A CLI tool for summarizing long PDFs, including a config file so the hiring manager could test different models and chunking strategies
- Time limit: Not specified (take-home)
- Company: AI startup (unnamed)
- Tools/constraints: Open choice
- Evaluators look for: Polish, documentation, configurability. The candidate who built this had two competing offers within 72 hours
- Source: [Medium - Fonzi AI / Sammi Cox](https://medium.com/fonzi-ai/what-ive-learned-from-sitting-in-on-50-ai-engineer-interviews-c493696453c4)

5. AI-powered feature or tool using an LLM API
- What to build: Choice of (A) building a small AI-powered feature or tool (intelligent search, summarization, validation, or assistant-style workflow) using an LLM API, or (B) contributing a small, scoped AI-related pull request or prototype demonstrating applied AI integration in an existing codebase
- Time limit: 2-3 hours (time-boxed)
- Company: FlowFuse
- Tools/constraints: AI tools explicitly allowed and encouraged
- Evaluators look for: Problem understanding, AI design decisions and tradeoffs, system structure, reliability considerations, how the solution would evolve over time, explicit discussion of where and why AI was used. 60-min technical interview reviewing the work with 2-3 team members
- Source: Job description analysis, FlowFuse Full Stack Developer (AI-focused) listing


### Agents & Workflows

6. AI agent demonstrating agentic behavior and reasoning
- What to build: Build an AI agent that demonstrates natural interaction, agentic behavior, clear reasoning steps, and strong technical decision-making. Candidates were given two assignment options and asked to choose one
- Time limit: 3-day window (given Nov 21, due Nov 24 at 2 PM)
- Company: Eightfold.ai (Agentic AI Engineering Intern)
- Tools/constraints: Open choice; candidate submitted "PulseAI" project
- Evaluators look for: Agentic behavior, reasoning clarity, alignment between personal AI projects and the role
- Source: [Medium - Tushar Bhardwaj](https://medium.com/@bhardwajtushar2004/inside-eightfold-ais-agentic-ai-internship-hiring-process-2026-f86dcb625aa8)

7. Agent with CSV data for personalized email campaigns
- What to build: Build an agent that reads customer CSV data and generates personalized email campaigns with evaluation metrics
- Time limit: Not specified (take-home)
- Company: Recommended by PromptLayer as standard AI engineer assessment
- Tools/constraints: Open choice
- Evaluators look for: Working functionality, thoughtful architecture, meaningful evaluation implementation
- Source: [PromptLayer blog](https://blog.promptlayer.com/the-agentic-system-design-interview-how-to-evaluate-ai-engineers/)

8. Conversational calendar booking agent
- What to build: A conversational calendar booking agent with LangGraph for orchestration, Streamlit chat interface, FastAPI backend, and Google Calendar integration. Must handle function calling, state management, and natural language scheduling
- Time limit: 2-3 days
- Company: Not specified (reported by candidates on Reddit/blogs)
- Tools/constraints: LangGraph, Streamlit, FastAPI, Google Calendar API
- Evaluators look for: Function calling implementation, state management, natural language understanding, API integration
- Source: Candidate reports aggregated from Reddit and interview discussion sites

9. AI-first CRM module with agentic workflow
- What to build: React/Redux frontend, FastAPI backend, LangGraph with 5+ tools. Deliverable includes GitHub repo plus a 10-15 minute demo video
- Time limit: ~60 hours expected
- Company: Not specified (AI startup)
- Tools/constraints: React, Redux, FastAPI, LangGraph, Docker
- Evaluators look for: Full-stack implementation, agentic tool orchestration, demo presentation quality
- Source: Candidate reports aggregated from Reddit and interview discussion sites

10. LLM agent for financial report analysis with stock price charts
- What to build: Build an LLM agent to ingest years of financial reports, with stock price analysis and chart generation - using only freemium APIs
- Time limit: 72 hours (Round 1 assignment)
- Company: Unnamed (candidate withdrew, calling it "an unpaid mini-consulting project")
- Tools/constraints: Must use only freemium APIs
- Evaluators look for: Not completed - candidate withdrew due to unreasonable scope for unpaid work
- Source: Reddit r/developpeurs, Jan 2026


### Code Review & Refactoring

11. Refactor convoluted code for maintainability (OpenAI onsite)
- What to build: Given roughly 100-120 lines of intentionally convoluted, deeply nested code that works and passes its test cases. Refactor it for long-term maintainability while keeping existing tests green and extending to new test cases representing incoming requirements
- Time limit: ~45-60 minutes (onsite round)
- Company: OpenAI
- Tools/constraints: AI tools are allowed during coding rounds (must share screen and narrate). Cannot dump the entire problem into ChatGPT and paste output - they watch for reasoning and judgment
- Evaluators look for: Architectural judgment, code quality, ability to balance immediate functionality with long-term thinking. Described by one candidate as "one of the better rounds" because it tests judgment that AI tools struggle with
- Source: [Exponent/Medium - Jacob Simon](https://medium.com/exponent/what-its-actually-like-to-interview-at-openai-in-2026-03a646c9436c)

12. Code review agent for Python files
- What to build: Implement a code review agent that analyzes Python files and provides actionable feedback
- Time limit: Not specified (take-home)
- Company: Recommended by PromptLayer as standard AI engineer assessment
- Tools/constraints: Open choice
- Evaluators look for: Working functionality, thoughtful architecture, meaningful evaluation implementation
- Source: [PromptLayer blog](https://blog.promptlayer.com/the-agentic-system-design-interview-how-to-evaluate-ai-engineers/)


### Data Processing & Pipelines

13. JSON parsing + AI summarization speed test (IBM)
- What to build: Parse a complicated JSON file to extract a specific part following a pattern, then feed that extracted data to an AI model to get a summary
- Time limit: 30 minutes (live, during managerial round)
- Company: IBM (AI Engineer, Watsonx team)
- Tools/constraints: Browser and ChatGPT explicitly allowed by the interviewer
- Evaluators look for: Speed, practical problem-solving under time pressure, ability to use tools effectively. The candidate got stuck at one point but kept going and finished in time
- Source: [Medium - Khushal Kumar](https://medium.com/@khushalkumar/my-generative-ai-engineer-interview-experience-got-hired-f8a027e070b0)

14. OpenAI progressive coding assessment (credits management system)
- What to build: A credits management system - OpenAI grants credits to customers with different expiration rules and usage requirements. Write algorithms to track credit state across a series of issued and used credits. Four progressive gates of increasing difficulty; clearing two gates is the passing bar
- Time limit: ~60 minutes (coding screen)
- Company: OpenAI
- Tools/constraints: AI tools allowed (share screen, narrate reasoning). Problems are bespoke - not found on LeetCode. Difficulty starts at medium, escalates through 4 gates
- Evaluators look for: Ability to reason through novel problems methodically; strong fundamentals over memorized patterns
- Source: [Exponent/Medium - Jacob Simon](https://medium.com/exponent/what-its-actually-like-to-interview-at-openai-in-2026-03a646c9436c)

15. Anthropic online assessment: progressive database operations
- What to build: One question divided into four progressive levels. Level 1: basic database operations (SET, GET, DELETE) managing key-field-value pairs. Level 2: SCAN and SCAN_BY_PREFIX operations returning formatted field lists in alphabetical order. Level 3: timestamped operations and TTL (time-to-live), requiring backward compatibility. Level 4: file compression/decompression with user storage management and ownership validation
- Time limit: 90 minutes (online assessment)
- Company: Anthropic
- Tools/constraints: Must be completed on coding platform; each level builds on previous code, demanding extensible design from the start
- Evaluators look for: Code extensibility, backward compatibility, correctness under increasingly complex requirements
- Source: [LinkJob.ai - Anthropic interview](https://www.linkjob.ai/interview-questions/anthropic-software-engineer-interview/)

16. Real-world integration/tooling project (Column Tax)
- What to build: A project that mirrors the kind of integration or tooling work the company does every day. Full freedom to do it your way
- Time limit: ~3 hours
- Company: Column Tax (Software Engineer, Applied AI)
- Tools/constraints: Open - "this is about decision-making, clarity, and implementation, not code golf"
- Evaluators look for: Decision-making quality, clarity, implementation approach; discussed during 4.5-hour interview panels over 2 days
- Source: Job description analysis, Column Tax listing


### ML Model Tasks (Training, Fine-Tuning, Evaluation)

17. Anthropic performance optimization take-home (now open-sourced)
- What to build: Optimize a kernel implementation for a simulated VLIW SIMD processor to minimize clock cycles. The task is a parallel tree traversal inspired by branchless SIMD decision tree inference. Baseline starts at 18,532 cycles (2-hour version) or 147,734 cycles (original 4-hour version)
- Time limit: Originally 4 hours, later reduced to 2 hours. Now open-sourced with unlimited time
- Company: Anthropic
- Tools/constraints: Python; cannot modify the tests/ folder; multicore support intentionally disabled. Candidates choose whether to tackle SIMD vectorization or VLIW instruction packing. Validated via `python tests/submission_tests.py`
- Evaluators look for: Performance optimization skills. Benchmark: Claude Opus 4.5 achieved 1,487 cycles (99x speedup from baseline); best human performance "substantially better." If you beat 1,487 cycles, Anthropic wants to hear from you (performance-recruiting@anthropic.com)
- Source: [GitHub - anthropics/original_performance_takehome](https://github.com/anthropics/original_performance_takehome)

18. Improve a sentiment classifier while maintaining interpretability (OpenAI-style)
- What to build: Given a sentiment classifier with 80% baseline accuracy, improve it while maintaining interpretability. Emphasis on sarcasm detection edge cases
- Time limit: Not specified (take-home)
- Company: OpenAI (reported by InterviewNode)
- Tools/constraints: Open choice; candidates used BERT, Sentence Transformers for embeddings
- Evaluators look for: Diagnostic analysis of misclassifications, feature engineering with embeddings, trade-off analysis between performance and explainability, bias detection. Best submissions framed improvements as scientific hypotheses
- Source: [InterviewNode](https://interviewnode.com/post/cracking-ml-take-home-assignments-real-examples-and-best-practices)

19. User retention prediction model (Meta-style)
- What to build: Build a model to predict whether a user will remain active after 30 days, given anonymized engagement data for a social media app
- Time limit: Not specified (take-home)
- Company: Meta (reported by InterviewNode)
- Tools/constraints: Anonymized dataset provided; logistic regression or gradient boosting recommended
- Evaluators look for: Problem definition clarity, EDA and behavioral distribution analysis, model selection, metric choice tied to business impact, communication in plain English. Winners connected model metrics to user engagement strategies rather than chasing raw accuracy
- Source: [InterviewNode](https://interviewnode.com/post/cracking-ml-take-home-assignments-real-examples-and-best-practices)

20. Demand forecasting model (Amazon-style)
- What to build: Forecast product demand for the next quarter using historical sales and marketing data
- Time limit: Not specified (take-home)
- Company: Amazon (reported by InterviewNode)
- Tools/constraints: Historical sales, marketing spend, seasonal factors dataset provided
- Evaluators look for: Feature engineering (lag features, rolling averages, calendar encoding), model interpretability via SHAP values, business-aware metrics (MAPE, RMSE with overstock/understock trade-off), scalability considerations. Top submissions treated code as production-ready with structured folders (src/, notebooks/) and comprehensive README
- Source: [InterviewNode](https://interviewnode.com/post/cracking-ml-take-home-assignments-real-examples-and-best-practices)

21. Anthropic alignment research exploration
- What to build: Given access to Anthropic's API, explore the system "like it was a mysterious black box" inspired by some research or blog post, then present findings
- Time limit: 5 hours, followed by a presentation call
- Company: Anthropic (alignment-focused role)
- Tools/constraints: Jupyter Notebook provided; Anthropic API access
- Evaluators look for: Research creativity, ability to form and test hypotheses about model behavior, presentation quality
- Source: [Blog - Goncharov](https://blog.goncharov.page/i-failed-my-anthropic-interview-and-came-to-tell-you-all-about-it-so-you-dont-have-to)

22. Reproduce a research paper or write pseudo-working implementation
- What to build: At some US-based AI startups, candidates interviewing for AI Research Engineer roles were asked not just to report and critique research papers, but to reproduce one of the assigned papers or write a pseudo-working implementation of the problem solved in the paper
- Time limit: Not specified (take-home)
- Company: US-based AI startups (unnamed)
- Tools/constraints: Not specified
- Evaluators look for: Ability to go from reading a paper to implementing its core ideas; practical research engineering skills
- Source: [Medium - Deepthi Sudharsan](https://medium.com/@deepthi.sudharsan/inside-ai-interviews-stories-patterns-and-what-actually-matters-555684c38598)


### Live Coding / Speed Tests

23. OpenAI 48-hour take-home project
- What to build: Not described in detail (project-based, not puzzle-based), followed by virtual onsite with versioned key-value store, One-NN implementation, feedforward neural network implementation, transformer bug-fixing (position embedding and KV cache issues), and PyTorch code completion
- Time limit: 48-hour window for take-home; virtual onsite is 5 rounds in one day
- Company: OpenAI
- Tools/constraints: Take-home is async; onsite coding problems are practical, not puzzle-based
- Evaluators look for: "Relentless pursuit of depth," real-world problem approach, how candidates think through open-ended problems step by step
- Source: [LinkJob.ai - OpenAI loop interview](https://www.linkjob.ai/interview-questions/openai-loop-interview)

24. Design the OpenAI Playground (system design with frontend/UX focus)
- What to build: Design the OpenAI Playground - specifically the feature that lets developers simulate full conversations and threads. Focus on frontend and UX design: drawing wireframes, thinking through developer workflow, reasoning about product questions like thread history, remixing past conversations, and making the API intuitive. Backend infrastructure and model serving are abstracted away
- Time limit: ~45-60 minutes (phone screen)
- Company: OpenAI
- Tools/constraints: System design with product thinking; interviewers do not volunteer information, candidates must ask explicitly
- Evaluators look for: Product thinking blended with technical architecture, scope management, clarifying questions, UX reasoning
- Source: [Exponent/Medium - Jacob Simon](https://medium.com/exponent/what-its-actually-like-to-interview-at-openai-in-2026-03a646c9436c)

25. Convert synchronous report generation to async (live coding with AI agent)
- What to build: Given a gRPC service with a synchronous report generation process causing client timeouts, diagnose the bottleneck using an AI coding agent (Claude Code), then implement the conversion to asynchronous processing. Follow-up: handle failure scenarios, scaling to 100 concurrent batch requests
- Time limit: ~45 minutes (live)
- Company: Unnamed (Exponent mock interview, but modeled on real-world interview patterns emerging in 2025-2026)
- Tools/constraints: AI coding agent (Claude Code) explicitly provided and expected to be used; Java/Spring Boot codebase
- Evaluators look for: Three "gates": (1) understanding that async is the solution, (2) knowing where to place the async call, (3) handling scale with multi-threading/queues. Also evaluated: how candidates use AI responsibly - understanding before implementing, not blindly pasting AI output
- Source: [Exponent YouTube mock interview](https://www.youtube.com/watch?v=exponent-ai-coding-mock)

26. Real-world case study project (Melotech)
- What to build: A real-world case study project that showcases your skills and working style. First step in the process - no screen or intro call before this
- Time limit: Not specified
- Company: Melotech (AI/ML Engineer + Intern)
- Tools/constraints: Not specified
- Evaluators look for: Results are presented and debated in a 90-minute case interview; same rigorous process for both senior and intern roles
- Source: Job description analysis, Melotech listing


### Full-Stack AI Apps

27. Build a project with Roboflow and present to CTO
- What to build: Build a project using Roboflow's product (computer vision platform), then present it in a 45-minute session to the company's CTO
- Time limit: ~45 minutes for presentation (prep time unspecified but advance preparation expected)
- Company: Roboflow (Full Stack Engineer, AI Agents)
- Tools/constraints: Must use Roboflow's own product
- Evaluators look for: Direct presentation to CTO; ability to build with the company's product, explain design choices, and demonstrate understanding of computer vision workflows
- Source: Job description analysis, Roboflow listing

28. ML/AI challenge showing pipeline thinking (boam)
- What to build: Solve a real Boam-style ML/AI challenge that shows your modeling approach, pipeline thinking, and execution muscle
- Time limit: Not specified
- Company: boam (Applied AI Engineer)
- Tools/constraints: Not specified
- Evaluators look for: Modeling approach, pipeline thinking, execution capability
- Source: Job description analysis, boam listing


### Paid Work Trials

29. PostHog SuperDay (1-day paid work trial)
- What to build: A full day of real work with the team, paid
- Time limit: 1 day
- Company: PostHog
- Tools/constraints: Work alongside the actual team on real problems
- Evaluators look for: How candidates work in practice, collaboration, code quality in a real environment
- Source: Job description analysis

30. Lorikeet ~2-day paid work trial
- What to build: Ship with the team for approximately 2 days, paid
- Time limit: ~2 days
- Company: Lorikeet
- Tools/constraints: Work with the actual engineering team
- Evaluators look for: Ability to ship real code in a team context
- Source: Job description analysis

31. Infinity Constellation 1-week paid trial (replaces all traditional interviews)
- What to build: Build something real for a full week, paid. This replaces all traditional interview formats
- Time limit: 1 week
- Company: Infinity Constellation
- Tools/constraints: Not specified
- Evaluators look for: Real output over interview performance
- Source: Job description analysis

32. Fifth Dimension 1-week paid trial (alternative track)
- What to build: Paid 1-week trial as an alternative to the standard interview pipeline
- Time limit: 1 week
- Company: Fifth Dimension
- Tools/constraints: Not specified
- Evaluators look for: Work output during the trial period
- Source: Job description analysis

33. CompuGroup Medical US 3-month stipended internship
- What to build: 3-month stipended internship with performance-based conversion to full-time
- Time limit: 3 months
- Company: CompuGroup Medical US
- Tools/constraints: Extended evaluation period
- Evaluators look for: Sustained performance and team fit over an extended period
- Source: Job description analysis


### System Design Presentations (Hybrid Take-Home + Live)

34. Present your most technically challenging project (OpenAI)
- What to build: Prepare slides and present your most technically challenging past project in a 45-minute discussion with a peer engineer. Not designing something new - showing deep ownership of something you built
- Time limit: 45 minutes (presentation + Q&A)
- Company: OpenAI (mandatory for L5/Staff+ roles)
- Tools/constraints: Screen-shared slides; choose a recent project where you drove technical strategy. Interviewers probe storage choices, model selection for inference, evaluation frameworks ("Is there an actual eval framework, or is it vibes-based?")
- Evaluators look for: Ownership, technical depth on your own work, greenfield and scale evidence, ability to discuss alternatives and tradeoffs for your own architecture decisions
- Source: [Exponent/Medium - Jacob Simon](https://medium.com/exponent/what-its-actually-like-to-interview-at-openai-in-2026-03a646c9436c), [Hello Interview - OpenAI L5 guide](https://www.hellointerview.com/guides/openai/l5)


### Additional Assignments from Candidate Reports

35. Login page with validations
- What to build: Create a login page accepting email and password with basic validations
- Time limit: 2-3 hours within a 2-3 day window
- Company: Not specified (entry-level)
- Tools/constraints: Not specified
- Evaluators look for: Basic full-stack skills, form validation, clean code
- Source: [dev.to - Aidi Rivera](https://dev.to/aidiri/learn-from-my-mistakes-my-first-take-home-code-challenge-778)

36. Customer support agent with eval-first approach (1.5-hour constraint)
- What to build: Create a customer support agent relevant to the company's product within 1.5 hours
- Time limit: 1.5 hours
- Company: YC startup (unnamed)
- Tools/constraints: Open choice; the key differentiator is whether the candidate starts with evals
- Evaluators look for: Red flag if candidate doesn't start with evals. Tests whether candidates build evaluation-first or code-first
- Source: [Reddit - What Is Your Interview Assignment for AI Engineers?](https://www.reddit.com/r/ycombinator/comments/1jnfijm/what_is_your_interview_assignment_for_ai_engineers/) (r/ycombinator)

37. Autonomous agent with observability/eval layer
- What to build: Build a simple autonomous agent using an open-source LLM with a task-specific goal and an observability/eval layer
- Time limit: Not specified
- Company: YC startup (unnamed)
- Tools/constraints: Must use open-source LLM; emphasis on observability and evaluation from the start
- Evaluators look for: Agent architecture, observability implementation, evaluation layer design
- Source: [Reddit - What Is Your Interview Assignment for AI Engineers?](https://www.reddit.com/r/ycombinator/comments/1jnfijm/what_is_your_interview_assignment_for_ai_engineers/) (r/ycombinator)

38. Evaluation tool for LLM hallucination detection
- What to build: Build an evaluation tool for LLM hallucination detection
- Time limit: Not specified
- Company: Not specified
- Tools/constraints: Not specified
- Evaluators look for: Understanding of hallucination types, evaluation methodology, practical detection approaches
- Source: [HN - Best Take-Home Coding Tasks](https://news.ycombinator.com/item?id=42182365)


### Summary Statistics

| Category | Count | Typical Time Range |
|----------|-------|-------------------|
| RAG & Document Q&A | 5 | 2-3 hours to 7 days |
| Agents & Workflows | 7 | 1.5 hours to ~60 hours |
| Code Review & Refactoring | 2 | 45-60 min (live) to take-home |
| Data Processing & Pipelines | 4 | 30 min (live) to 3 hours |
| ML Model Tasks | 6 | 2 hours to 5 hours |
| Evaluation | 1 | Varies |
| Live Coding / Speed Tests | 4 | 45 min to 48 hours |
| Full-Stack AI Apps | 3 | Varies |
| Paid Work Trials | 5 | 1 day to 3 months |
| System Design Presentations | 1 | 45 min |

### Key Patterns Across All Assignments

AI tools policy: The trend is shifting toward explicitly allowing AI tools. OpenAI allows AI during coding rounds (must narrate reasoning). IBM allowed browser + ChatGPT during a live speed test. FlowFuse explicitly encourages AI tools. Emerging interview formats (like the Exponent mock) now provide AI coding agents and evaluate how candidates use them.

What consistently differentiates top submissions:
- Working end-to-end solutions over partial perfection
- Documentation of design decisions and tradeoffs
- Loom/video walkthroughs of the submission
- Configurability (e.g., the PDF summarizer CLI with a config file)
- Cost awareness and optimization discussion
- Production-readiness signals (error handling, monitoring, deployment)
- Connecting technical metrics to business outcomes

Common pitfalls reported by evaluators:
- Rushing take-homes without documenting decisions
- Over-engineering (building more than asked for without justification)
- Ignoring evaluation/testing of AI outputs
- Treating the LLM as a black box without discussing failure modes
- Not asking clarifying questions before starting

## How to Prepare

### 9 lessons from a first take-home challenge

From a developer's retrospective on their first take-home code challenge ([source](https://dev.to/aidiri/learn-from-my-mistakes-my-first-take-home-code-challenge-778)):

1. Ask questions - asking clarifying questions after receiving the challenge demonstrates communication skills and prevents costly mistakes
2. Double your time estimate - Hofstadter's Law applies: work always takes longer than expected, even when you account for that
3. Hold yourself to an earlier deadline - tell the interviewer you need the full time, but privately commit to finishing sooner (Parkinson's Law: work expands to fill available time)
4. Emulate the company's brand - copy the company's existing style rather than spending excessive time on design
5. Stop searching for the "perfect" way - trust your knowledge and implement solutions you understand rather than endlessly seeking ideal approaches
6. Test your code - even if testing isn't explicitly required, include basic tests; interviewers often ask about them
7. Comment your code - clear, concise comments explaining function purposes and logic demonstrate your thought process
8. Stop doubting yourself - whether your code suffices is the interviewer's judgment, not yours
9. Don't spend excessive time - recognize when additional effort yields diminishing returns

Common mistakes:

- Rushing without documenting design decisions and trade-offs
- Over-engineering beyond what was asked without justification
- Ignoring evaluation and testing of AI outputs
- Treating the LLM as a black box without discussing failure modes
- Not asking clarifying questions before starting

### Using LLMs to practice for technical interviews

LLMs can generate targeted practice problems and provide detailed feedback on system design solutions. One candidate preparing for a Bay Area AI startup interview pasted the job description into Grok and asked it to generate relevant system design questions - it produced a company-specific problem ("Design an AI-powered Candidate Sourcing System") with functional requirements, non-functional constraints, and scale estimates. After sketching a solution on Excalidraw, the candidate pasted a screenshot back into the LLM with the prompt "give me critical and comprehensive feedback like a senior engineer who is interviewing me" and received detailed critique identifying fatal flaws (e.g., putting a message queue in the synchronous read path, HTTP violations, missing write path). Cross-checking answers between different LLMs improved the diversity of feedback and helped identify which details were more important ([source](https://levelup.gitconnected.com/how-i-fought-and-passed-technical-interviews-with-llms-in-2025-6adb9afe2401))
