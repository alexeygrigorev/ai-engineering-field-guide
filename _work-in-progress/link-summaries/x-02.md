# X/Twitter Link Summaries - Batch 02

---

## kmeanskaran - Indian IT Recruitment and ML Interview Dysfunction

**URL:** https://x.com/kmeanskaran/status/2021913598512242819
**Summary:** Karan criticizes Indian IT recruitment practices where HR interviewers with MBA backgrounds ask technical ML questions like "What is regularization?" and "How do you deploy a model?" without domain knowledge. He argues companies prioritize candidates who answer quickly and show up on time rather than seeking genuine problem-solvers, noting the irony of requiring multiple DSA rounds while never connecting candidates with actual ML team members.
**Interview Questions Mentioned:**
- "What is regularization?"
- "How do you deploy a model?"
**Key Insights:**
- HR gatekeepers with no ML background are screening for technical ML roles
- Speed and availability are weighted over problem-solving ability in many Indian IT hiring processes
- Candidates face DSA rounds as a filter even for ML roles, without subject-matter expert interviews
- The disconnect between recruiter capabilities and role requirements is a systemic issue

---

## MimansaJ - 200+ Applications, 100 Interviews: LLM/ML Job Search Fall 2024

**URL:** https://x.com/MimansaJ/status/1894075959399387321
**Summary:** Mimansa Jaiswal documents her extensive Fall 2024 LLM/ML research job search: over 200 applications, 100 interviews, many rejections, and ultimately some offers. She published a detailed guide covering job search mechanics, preparation materials, and industry insights, noting that the fast-moving LLM field means information evolves quickly. The post received 400K+ views and 4K likes.
**Interview Questions Mentioned:** None in the tweet itself; full details at mimansajaiswal.github.io/posts/llm-ml-job-interviews-fall-2024-process/
**Key Insights:**
- LLM/ML research roles are highly competitive — 200 applications to get some offers is a realistic ratio
- The job search itself is a numbers game requiring persistence through high rejection rates
- The LLM field moves fast enough that interview content and expectations shift year-to-year
- A documented, systematic approach to the search (tracking applications, resources, learnings) is valuable
- The blog post is a notable resource for LLM/ML research role interview preparation

---

## omarsar0 - Top 50 LLM Interview Questions Resource

**URL:** https://x.com/omarsar0/status/1930984834454712537
**Summary:** Elvis Saravia (@omarsar0, founder of DAIR.AI, ex-Meta AI) shares what he describes as a great resource: a curated list of 50 LLM interview questions for learning LLM basics. The post received 349K+ views and 2.7K likes, indicating strong community interest in structured LLM interview preparation materials.
**Interview Questions Mentioned:** References a collection of 50 LLM interview questions (image attached, specific questions not extracted from the API response)
**Key Insights:**
- Structured LLM interview question lists are in high demand in the AI community
- Even researchers with strong credentials (PhD, Meta AI, Elastic) are sharing interview prep resources
- The volume of engagement (349K views) reflects how many people are actively preparing for LLM roles
- DAIR.AI is a notable source for democratized AI education and interview prep resources

---

## OsokoyaF - Interview Thread: Prompt Engineering vs Fine-Tuning

**URL:** https://x.com/OsokoyaF/status/2016519979978436904
**Summary:** Osokoya Fiyin opens a thread directly framed as interview preparation, asking candidates to explain the difference between prompt engineering and fine-tuning. The thread is positioned as practical guidance for AI engineering interview candidates on a foundational conceptual distinction.
**Interview Questions Mentioned:**
- "Explain the difference between prompt engineering and fine-tuning."
**Key Insights:**
- Prompt engineering vs. fine-tuning is a common AI engineering interview question
- Understanding when to use each approach (cost, latency, data requirements, task complexity) is key
- The question tests both conceptual knowledge and practical judgment
- Posted in the "Build in Public" community, suggesting grassroots AI education content

---

## prateek_0041 - AI Is Creating Engineers with Shallow Technical Knowledge

**URL:** https://x.com/prateek_0041/status/1945150930330247442
**Summary:** Prateek Singh shares his hiring experience finding that candidates with polished AI-assisted resumes couldn't explain their own code or implement basic extensions in live interviews. He uses a two-stage interview process: an AI-permitted assignment followed by a code explanation and live coding session, revealing that many candidates with 2+ years of experience lack fundamentals like Docker, API fetching, CRUD, and testing basics.
**Interview Questions Mentioned:** Implied: code explanation of submitted assignment, live coding extensions, implementation of utility functions
**Key Insights:**
- AI tools allow candidates to produce polished submissions without understanding the underlying code
- Two-stage interviews (AI-allowed assignment + live explanation) are an effective filter
- Fundamentals expected from engineers with 2+ years: Docker, API fetching, CRUD, documentation, testing
- Interviewers are increasingly using "explain this code you wrote" as a verification step
- The author's advice to candidates: invest in deep technical knowledge, not AI shortcuts
- This is a warning signal for candidates who rely on AI without understanding the output

---

## Pseudo_Sid26 - AI/ML Career Roadmap: Ready vs Aspiration Roles

**URL:** https://x.com/Pseudo_Sid26/status/1983503905943380105
**Summary:** Siddharth maps his current job readiness across AI/ML roles: confident applying for Data Analyst, ML Engineer, Python Backend Engineer, and MLOps Engineer, while identifying Gen AI Engineer, AI Engineer, Applied AI Engineer, and Data Engineer as aspiration roles. He explicitly lists his skills gaps: Databricks, Terraform, LangGraph, LangChain, Agentic RAG, System Design, agent automation tools, and LLMOps.
**Interview Questions Mentioned:** None
**Key Insights:**
- Clear self-assessment of role readiness is a useful career planning framework
- Skills gap for senior AI/Gen AI roles in 2025: LangGraph, LangChain, Agentic RAG, LLMOps, System Design
- Databricks and Terraform appear as data engineering gaps even for AI-focused candidates
- "Applied AI Engineer" and "Agentic" roles require a distinct skill set beyond core ML engineering
- This kind of public skills mapping reflects the community norm of learning in public

---

## pvergadia - Nine-Step AI Engineering Interview Preparation Framework

**URL:** https://x.com/pvergadia/status/2001117645933043781
**Summary:** Priyanka Vergadia (@pvergadia) shares a comprehensive nine-area framework for AI engineering interview preparation, covering the full stack from data engineering and deep learning to model serving, optimization, and AI safety. The post received 544 likes, 612 bookmarks, and 26.5K views, suggesting it's widely used as a reference guide.
**Interview Questions Mentioned:** None as explicit questions, but the framework covers topics commonly tested in interviews
**Key Insights:**
- Nine areas to master for AI engineering interviews: data engineering (SQL, Pandas, Spark), deep learning (PyTorch), LLMs and prompt engineering (CoT, few-shot, LangChain), RAG systems (vector DBs, embeddings), MLOps/CI/CD (MLflow, W&B), fine-tuning (PEFT, LoRA, QLoRA), model serving (FastAPI, Triton, vLLM), optimization/quantization (FP16, INT8, pruning, distillation), AI safety and evaluation (RAGAS, hallucination detection)
- Key advice: "Understand the math, but master the implementation"
- Always question whether an LLM is actually necessary for the problem
- 612 bookmarks suggests this is heavily used for interview prep

---

## ramsri_goutham - LLM Interview Preparation: US vs India Differences

**URL:** https://x.com/ramsri_goutham/status/1747875498171171217
**Summary:** Ramsri Goutham contrasts LLM engineer interview preparation between the US (focus: fine-tuning Llama 2 with domain-specific datasets) and India (focus: building LLMs for Indic languages). Indian-specific preparation includes training custom tokenizers for Indic languages, continuous pretraining on bilingual text, creating SFT datasets for multiple language modes, and building systems handling multiple Indian language combinations.
**Interview Questions Mentioned:** None explicitly, but implied topics: fine-tuning Llama 2, tokenizer design for Indic languages, multilingual SFT dataset creation
**Key Insights:**
- LLM interview content differs significantly by geography and market
- India-focused LLM roles require knowledge of Indic language NLP: custom tokenizers, bilingual pretraining, Hinglish handling
- Translating English SFT datasets for Indian languages loses cultural context and is not recommended
- US-focused roles emphasize domain-specific fine-tuning with established English-language models
- This highlights the need to tailor preparation based on target market and company focus

---

## _rohit_tiwari_ - Top 50 LLM Interview Questions: Seven Topic Areas

**URL:** https://x.com/_rohit_tiwari_/status/1958163536619712770
**Summary:** Rohit Kumar Tiwari posts a structured breakdown of 50 LLM interview questions across seven categories: core concepts (tokenization, attention, context windows), training methods (LoRA, fine-tuning), text generation (decoding, prompting), math foundations (calculus, loss functions, linear algebra), architectures (transformers, MoE, RAG), applications (model comparisons, foundation models), and practical issues (hyperparameters, bias, deployment). The post received 46K+ views and 814 bookmarks.
**Interview Questions Mentioned:** References 50 LLM interview questions organized across seven topic areas (full PDF available via DM)
**Key Insights:**
- Seven core knowledge areas tested in LLM interviews: core concepts, training, generation, math, architectures, applications, practical issues
- Math foundations (calculus, linear algebra, loss functions) remain expected knowledge for LLM roles
- MoE (Mixture of Experts) is now a mainstream architecture topic in LLM interviews
- Bias mitigation and deployment challenges are included as practical interview topics
- 814 bookmarks indicates this is a heavily referenced interview prep resource

---

## safishamsii - Career Milestone: MS with Distinction + AI Engineer Offer in London

**URL:** https://x.com/safishamsii/status/2014249047138189496
**Summary:** Safi documents their early 2026 career milestones: completing an MS with distinction from the University of Birmingham, receiving multiple AI/ML job offers, and accepting an AI Engineer position at a VC-backed London startup. The post reflects on overcoming rejection through persistence and includes reflections on the job search journey.
**Interview Questions Mentioned:** None
**Key Insights:**
- An MS with distinction is a useful credential for breaking into AI Engineer roles at London startups
- Multiple offers are achievable but require persistence through a high rejection rate
- VC-backed startups are active hirers of new AI engineering graduates in the UK market
- The post reflects a broader pattern: strong academic credentials + persistence = entry into AI roles
- London is an active market for AI engineering roles in 2025-2026

---

## SakanaAILabs - Guide to Preparing for Research Role Applications

**URL:** https://x.com/SakanaAILabs/status/2013431501040263363
**Summary:** Sakana AI (Tokyo-based AI R&D lab) shares an unofficial preparation guide for research role applicants, emphasizing four principles: depth over breadth, questioning the problem space rather than just solving it, explaining reasoning transparently, and showing genuine curiosity rather than just technical competence. The guide targets candidates applying to research positions specifically.
**Interview Questions Mentioned:** None explicit, but the framework implies research interviews test: problem formulation, reasoning transparency, intellectual curiosity
**Key Insights:**
- Research role interviews value depth over broad surface-level knowledge
- Interviewers want candidates who question the problem, not just answer it
- Explaining reasoning process matters as much as reaching the correct answer
- Curiosity and intellectual engagement are evaluated alongside technical ability
- This contrasts with engineering interviews which tend to focus more on implementation
- Sakana AI's framing is useful for anyone targeting AI research roles (vs. applied engineering)

---

## sh_reya - Production ML Interview Study Paper in Progress

**URL:** https://x.com/sh_reya/status/1557924005310386181
**Summary:** Shreya Shankar (UC Berkeley EECS researcher, data systems and HCI) announces she is drafting a research paper focused on production machine learning interviews, expressing excitement about quotes she collected that are "better than what I could have ever made up." The paper was later published as a notable resource on production ML interview practices.
**Interview Questions Mentioned:** None in this tweet; the paper itself contains interview content
**Key Insights:**
- Academic research on production ML interview practices exists — Shreya Shankar is a key author
- The paper draws on real practitioner quotes, grounding it in actual industry experience
- Production ML interviews are distinct from standard ML/DS interviews and deserve dedicated preparation
- Shankar's work at the intersection of data systems and HCI informs her lens on production ML
- The paper (published after this tweet) is a notable resource for understanding what production ML roles actually test

---

## svpino - 16 ML Interview Questions (March 2021)

**URL:** https://x.com/svpino/status/1368231752494444549
**Summary:** Santiago Valdarrama shares 16 interview questions he likes to ask ML candidates, framed as preparation material for job seekers. The tweet garnered 1,332 likes, 321 retweets, and 1,100 bookmarks, making it a well-referenced ML interview prep resource from 2021.
**Interview Questions Mentioned:** References a list of 16 ML interview questions (full list in thread/image, not extracted from API response)
**Key Insights:**
- Santiago (@svpino) is a prolific creator of ML interview preparation content with 439K followers
- His interview question lists from 2021 remain bookmarked resources years later, indicating longevity of foundational ML concepts
- 1,100 bookmarks suggests sustained use as a reference
- The questions are framed from an interviewer's perspective, offering insight into what practitioners actually evaluate

---

## svpino - 20 Neural Network Interview Questions (June 2022)

**URL:** https://x.com/svpino/status/1533780753900634114
**Summary:** Santiago Valdarrama shares 20 practice questions for machine learning interviews focused on neural networks, covering fundamental concepts candidates should know. The post received 1,237 likes, 287 retweets, and 50 replies, reflecting strong engagement from the ML community.
**Interview Questions Mentioned:** References 20 neural network-focused ML interview questions (specific questions in thread/image)
**Key Insights:**
- Neural network fundamentals remain a core ML interview topic even as the field evolves toward LLMs
- Santiago consistently publishes interview question collections — this is a follow-up to earlier posts
- The focus on "fundamental concepts" signals that deep understanding of basics outweighs knowledge of the latest models
- 1,237 likes across a broad audience suggests these questions resonate as genuinely representative of interviews

---

## tom_doerr - 300 Real-World ML System Design Case Studies

**URL:** https://x.com/tom_doerr/status/1950807395027882240
**Summary:** Tom Dorr shares a curated list of 300 real-world ML system design case studies, positioning it as a practical resource for interview preparation and learning. The post received modest engagement (17 likes, 3 retweets, 25 bookmarks) but the resource itself — a large collection of production ML case studies — is substantively valuable.
**Interview Questions Mentioned:** None; the resource is a collection of case studies, not questions
**Key Insights:**
- 300 real-world case studies is a substantial resource for ML system design interview preparation
- Real case studies from production systems are more valuable than theoretical design exercises
- System design is a distinct interview stage for ML roles, requiring knowledge of production architectures
- Tom Dorr frequently shares GitHub repositories and curated resources for AI/ML practitioners

---

## va_a14 - LinkedIn ML Engineer Interview Process and Compensation (India)

**URL:** https://x.com/va_a14/status/2008516020215095419
**Summary:** Vaibhav Agarwal details LinkedIn's Machine Learning Engineer interview process and compensation in India (60+ LPA for 2+ years experience, 85+ LPA for senior candidates). The process spans five rounds: screening (DSA, probability, ML fundamentals), AI coding (mathematical problem-solving), DSA (two-pointer, array partitioning), AI fundamentals (embeddings, BERT, regularization), system design (cold-start recommender), and hiring manager behavioral.
**Interview Questions Mentioned:**
- DSA: two-pointer problems, array partitioning via backtracking
- AI fundamentals: embeddings, BERT, regularization techniques
- System design: design a cold-start recommender system
- Screening: probability questions, ML fundamentals
**Key Insights:**
- LinkedIn ML Engineer interviews in India require strong algorithmic skills alongside deep AI theory
- Cold-start recommender system design is a specific system design question used by LinkedIn
- BERT and embeddings are tested as AI fundamentals, not just as LLM topics
- The 5-round structure is: screening + AI coding + DSA + AI fundamentals + system design + behavioral
- Compensation benchmarks: 60+ LPA (2+ years), 85+ LPA (senior) in India

---

## verrsane - Recruiter Role Mismatch: AI Engineer vs ML Engineer

**URL:** https://x.com/verrsane/status/2011562947617939957
**Summary:** An AI engineer (verrsane) describes a frustrating recruiting experience where a recruiter couldn't provide a job description, claimed the role was for an "AI Engineer," but the actual interview turned out to be for an ML Engineer position. The post highlights the persistent confusion between AI Engineer and ML Engineer role definitions in hiring.
**Interview Questions Mentioned:** None
**Key Insights:**
- The AI Engineer vs ML Engineer distinction remains unclear to many recruiters
- Recruiters operating without job descriptions is a common and frustrating pattern
- Role title inflation and mismatch between recruiter descriptions and actual interview scope is widespread
- Candidates should push for job descriptions and role clarity before investing time in interview processes
- This confusion creates friction for candidates trying to target their preparation appropriately
