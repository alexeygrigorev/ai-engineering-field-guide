company:
  name: Airbnb
  stage: Public
  focus: Online marketplace for travel and lodging
position:
  title: Senior Staff Machine Learning Engineer - AI Safety and Guardrail
  ai_type:
    type: ai-support
    reasoning: |-
      The role focuses on building guardrails, safety mechanisms,
      monitoring systems, and evaluation frameworks for AI systems
      rather than directly building, training, or fine-tuning
      models. While the engineer works closely with AI systems and
      understands LLM concepts, their primary responsibility is
      implementing safety infrastructure and compliance measures
      around AI developed by other teams.
  responsibilities:
  - Collaborate with cross-functional teams to identify issues, evaluate risks, design
    monitoring systems, and deploy efficient solutions for safe AI adoption
  - Design and implement guardrails to mitigate risks like hallucinations, privacy
    breaches, prompt injections, harmful responses, or bias
  - Set up continuous risk monitoring pipelines and alerting to enable human-in-the-loop
    feedback and mitigation
  - Partner with evaluation and data platform teams to build data flywheel for fixing
    model failure modes and guardrail improvement
  - Collaborate with trust, security, legal and operation teams to enable risk management
    and ensure compliance with privacy standards
  - Partner with ML infrastructure to scale safety features across Airbnb products
    and maintain documentation for auditability
  use_cases:
  - AI-powered chatbots for customer support providing intelligent responses to guest
    and host inquiries
  - AI Assistants that help customers navigate support interactions and resolve issues
    efficiently
  - Agent Copilot systems that augment human support agents with AI-generated suggestions
    and insights
  - Community Support x Artificial Intelligence (CSxAI) initiatives enabling scalable
    and exceptional service experiences
  - RAG/Search systems that retrieve relevant information to improve AI response accuracy
    and contextual understanding
  - Automated evaluation and testing systems to continuously monitor AI model performance
    and safety compliance
  skills:
    genai: [LLM fine-tuning, RAG, SFT, RLHF, DPO, LLM evaluation, prompt engineering,
      LLM alignment techniques, guardrails]
    ml: [model training, machine learning frameworks]
    web: []
    databases: []
    data: []
    cloud: []
    ops: []
    languages: [Python]
    domains: [content safety, ML fairness and bias, responsible AI, AI model security]
    other: []
  is_customer_facing: false
  is_management: false
meta:
  job_id: '6860554'
  extracted_at: '2026-02-05T09:56:10.639322'
